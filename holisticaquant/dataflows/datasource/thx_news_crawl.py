"""
åŒèŠ±é¡ºæ–°é—»çˆ¬è™«æ•°æ®æº

ä»åŒèŠ±é¡ºè´¢ç»ç½‘ç«™çˆ¬å–æ–°é—»æ•°æ®ï¼Œè¿”å›ç»Ÿä¸€æ ¼å¼
"""
import asyncio
import requests
import json
import re
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import time
import random
from loguru import logger

from .data_source_base import DataSourceBase


class ThxNewsCrawl(DataSourceBase):
    """
    åŒèŠ±é¡ºæ–°é—»çˆ¬è™«
    
    ä»åŒèŠ±é¡ºè´¢ç»ç½‘ç«™è·å–æ–°é—»æ•°æ®ï¼Œè¿”å›ç»Ÿä¸€æ ¼å¼çš„ DataFrame
    """
    
    def __init__(self, max_pages: int = 5, enable_frontend_crawl: bool = False,
                 max_size_kb: Optional[float] = 512.0,
                 max_time_range_days: Optional[int] = 7,
                 max_records: Optional[int] = 500,
                 use_cache: bool = True):
        """
        Args:
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
            enable_frontend_crawl: æ˜¯å¦å¯ç”¨å‰ç«¯çˆ¬å–ï¼ˆéœ€è¦ crawl4aiï¼Œé»˜è®¤å…³é—­ï¼‰
            max_size_kb: æœ€å¤§æ•°æ®å¤§å°ï¼ˆKBï¼‰
            max_time_range_days: æœ€å¤§æ—¶é—´èŒƒå›´ï¼ˆå¤©æ•°ï¼‰
            max_records: æœ€å¤§è®°å½•æ•°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ŒFalse è¡¨ç¤ºæ¯æ¬¡éƒ½è·å–æœ€æ–°æ•°æ®ï¼ˆé€‚åˆLLMè°ƒç”¨ï¼‰ï¼Œé»˜è®¤ True
        """
        super().__init__("thx_news_crawl", max_size_kb=max_size_kb,
                        max_time_range_days=max_time_range_days,
                        max_records=max_records, use_cache=use_cache)
        self.max_pages = max_pages
        self.enable_frontend_crawl = enable_frontend_crawl

    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        text = re.sub(r"<[^>]+>", " ", text)
        text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)
        text = re.sub(r"\s+", " ", text).strip()
        return text

    def parse_pub_time_from_frontend(self, line: str, url: str) -> str:
        """ä»å‰ç«¯é¡µé¢è§£æå‘å¸ƒæ—¶é—´"""
        try:
            date_part = None
            m_url_date = re.search(r"/(\d{8})/", url or "")
            if m_url_date:
                ymd = m_url_date.group(1)
                date_part = f"{ymd[0:4]}-{ymd[4:6]}-{ymd[6:8]}"
            time_part = None
            m_time = re.search(r"\b(\d{1,2}):(\d{2})\b", line or "")
            if m_time:
                h = int(m_time.group(1))
                m = int(m_time.group(2))
                if 0 <= h <= 23 and 0 <= m <= 59:
                    time_part = f"{h:02d}:{m:02d}:00"
            if not date_part:
                m_cn = re.search(r"(\d{4})[å¹´/\\-](\d{1,2})[æœˆ/\\-](\d{1,2})", line or "")
                if m_cn:
                    y = int(m_cn.group(1))
                    mo = int(m_cn.group(2))
                    d = int(m_cn.group(3))
                    date_part = f"{y:04d}-{mo:02d}-{d:02d}"
            if date_part and time_part:
                return f"{date_part} {time_part}"
            if date_part:
                return f"{date_part} 00:00:00"
            return ""
        except Exception:
            return ""

    def extract_company_news_from_markdown(self, md: str) -> List[Dict[str, Any]]:
        """ä» Markdown æ ¼å¼æå–å…¬å¸æ–°é—»"""
        records = []
        for raw_line in (md or "").splitlines():
            line = raw_line.strip()
            if not (line.startswith('* ') or line.startswith('- ')):
                continue
            m_link = re.search(r"\[([^\]]+)\]\((https?://[^)\s]+)[^)]*\)", line)
            if not m_link:
                continue
            title = (m_link.group(1) or "").strip()
            url = (m_link.group(2) or "").strip()
            if not re.search(r"/(\d{8})/", url):
                continue
            tail = line[m_link.end():]
            m_intro = re.search(r"\[([^\]]+)\]", tail)
            intro = (m_intro.group(1) or "").strip() if m_intro else ""
            content = self.clean_text(intro)
            pub_time = self.parse_pub_time_from_frontend(line, url)
            records.append({
                "title": title or "",
                "content": content or "",
                "pub_time": pub_time or "",
                "url": url or "",
            })
        return records

    def clean_html_content(self, html_content: str) -> str:
        """æ¸…ç†HTMLå†…å®¹"""
        if not html_content:
            return ""
        clean_text = re.sub(r'<[^>]+>', '', html_content)
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        clean_text = clean_text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
        return clean_text

    def parse_pub_time(self, timestamp: int) -> str:
        """è§£ææ—¶é—´æˆ³ä¸ºå­—ç¬¦ä¸²"""
        try:
            dt = datetime.fromtimestamp(timestamp)
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        except:
            return ""

    def get_news_data(self, page: int = 1, pagesize: int = 400) -> List[Dict[str, Any]]:
        """è·å–æ–°é—»æ•°æ®ï¼ˆAPIæ–¹å¼ï¼‰"""
        url = "https://news.10jqka.com.cn/tapp/news/push/stock/"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'Referer': 'https://news.10jqka.com.cn/realtimenews.html',
            'Origin': 'https://news.10jqka.com.cn',
            'Connection': 'keep-alive',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'X-Requested-With': 'XMLHttpRequest'
        }
        
        params = {
            'page': page,
            'tag': '',
            'track': 'website',
            'pagesize': pagesize
        }
        
        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            news_list = data.get('data', {}).get('list', [])
            processed_news = []
            for news in news_list:
                title = news.get('title', '')
                content = self.clean_html_content(news.get('digest', '')) 
                pub_time = self.parse_pub_time(int(news.get('ctime', 0))) 
                news_url = news.get('url', '')
                
                if not news_url and news.get('id'):
                    news_url = f"https://news.10jqka.com.cn/tapp/news/push/stock/{news.get('id')}/"
                
                processed_news.append({
                    'title': title,
                    'content': content,
                    'pub_time': pub_time,
                    'url': news_url
                })
            
            return processed_news
            
        except requests.exceptions.RequestException as e:
            logger.error(f"è¯·æ±‚å¤±è´¥: {e}")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            return []
        except Exception as e:
            logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
            return []

    def crawl_multiple_pages(self) -> List[Dict[str, Any]]:
        """çˆ¬å–å¤šé¡µæ•°æ®"""
        all_news = []
        
        for page in range(1, self.max_pages + 1):
            page_news = self.get_news_data(page=page, pagesize=400)
            
            if not page_news:
                break
                
            all_news.extend(page_news)
            # ä¼˜åŒ–ï¼šç§»é™¤äººä¸ºå»¶è¿Ÿä»¥æå‡æ€§èƒ½
            # å¦‚æœéœ€è¦é™æµï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨å¼‚æ­¥sleep: await asyncio.sleep(0.1)
            # if page < self.max_pages:
            #     delay = random.uniform(1, 3)
            #     time.sleep(delay)
        
        return all_news

    async def crawl_frontend_pages(self) -> List[Dict[str, Any]]:
        """å‰ç«¯çˆ¬å–ï¼ˆéœ€è¦ crawl4aiï¼‰"""
        if not self.enable_frontend_crawl:
            logger.info("å‰ç«¯çˆ¬å–å·²ç¦ç”¨")
            return []
        
        try:
            # å°è¯•å¯¼å…¥ crawl4ai
            try:
                from crawl4ai import AsyncWebCrawler
            except ImportError:
                logger.warning("crawl4ai æœªå®‰è£…ï¼Œè·³è¿‡å‰ç«¯çˆ¬å–")
                return []
            
            async with AsyncWebCrawler() as crawler:
                company_news_urls = [
                    "https://stock.10jqka.com.cn/companynews_list/index.shtml",
                    *[f"https://stock.10jqka.com.cn/companynews_list/index_{i}.shtml" for i in range(2, 21)],
                ]
                
                hsdp_urls = [
                    "https://stock.10jqka.com.cn/hsdp_list/index.shtml",
                    *[f"https://stock.10jqka.com.cn/hsdp_list/index_{i}.shtml" for i in range(2, 21)],
                ]
                
                page_urls = company_news_urls + hsdp_urls
                logger.info(f"å¼€å§‹å‰ç«¯çˆ¬å–ï¼Œå…± {len(page_urls)} é¡µ")
                
                results = await crawler.arun_many(urls=page_urls)
                logger.info(f"å‰ç«¯çˆ¬å–å®Œæˆï¼Œå¤„ç† {len(results or [])} ä¸ªå“åº”")
                
                all_records = []
                for res in (results or []):
                    page_markdown = getattr(res, "markdown", "")
                    all_records.extend(self.extract_company_news_from_markdown(page_markdown))
                
                logger.info(f"å‰ç«¯çˆ¬å–å®Œæˆï¼Œè·å– {len(all_records)} æ¡è®°å½•")
                return all_records
                
        except Exception as e:
            logger.error(f"å‰ç«¯çˆ¬å–å¤±è´¥: {e}")
            return []

    async def get_data(self, trigger_time: str, **query_params) -> pd.DataFrame:
        """
        å®ç°åŸºç±»çš„æŠ½è±¡æ–¹æ³•ï¼Œè·å–æ•°æ®
        
        Args:
            trigger_time: è§¦å‘æ—¶é—´å­—ç¬¦ä¸²
            
        Returns:
            DataFrame with columns ['title', 'content', 'pub_time', 'url']
        """
        tasks = [
            asyncio.to_thread(self.crawl_multiple_pages),  # APIçˆ¬å–
            self.crawl_frontend_pages()  # å‰ç«¯çˆ¬å–
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†APIçˆ¬å–ç»“æœ
        api_news_data = []
        if isinstance(results[0], list):
            api_news_data = results[0]
            logger.info(f"âœ… APIçˆ¬å–æˆåŠŸ: {len(api_news_data)} æ¡è®°å½•")
        elif isinstance(results[0], Exception):
            logger.error(f"âŒ APIçˆ¬å–å¤±è´¥: {results[0]}")
        else:
            logger.warning(f"âš ï¸ APIçˆ¬å–è¿”å›æ„å¤–ç±»å‹: {type(results[0])}")
        
        # å¤„ç†å‰ç«¯çˆ¬å–ç»“æœ
        frontend_news_data = []
        if isinstance(results[1], list):
            frontend_news_data = results[1]
            logger.info(f"âœ… å‰ç«¯çˆ¬å–æˆåŠŸ: {len(frontend_news_data)} æ¡è®°å½•")
        elif isinstance(results[1], Exception):
            logger.error(f"âŒ å‰ç«¯çˆ¬å–å¤±è´¥: {results[1]}")
        else:
            logger.warning(f"âš ï¸ å‰ç«¯çˆ¬å–è¿”å›æ„å¤–ç±»å‹: {type(results[1])}")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        all_news_data = api_news_data + frontend_news_data
        
        logger.info(f"ğŸ“ˆ æ•°æ®æ”¶é›†æ±‡æ€»: API {len(api_news_data)} æ¡, å‰ç«¯ {len(frontend_news_data)} æ¡, åˆè®¡ {len(all_news_data)} æ¡")
        
        # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªæ•°æ®æºæˆåŠŸ
        if not api_news_data and not frontend_news_data:
            logger.error("âŒ APIå’Œå‰ç«¯çˆ¬å–éƒ½å¤±è´¥ï¼Œæ— å¯ç”¨æ•°æ®")
            return pd.DataFrame(columns=self.REQUIRED_COLUMNS)
        elif not all_news_data:
            logger.warning("âš ï¸ æœªæ”¶é›†åˆ°ä»»ä½•æ•°æ®")
            return pd.DataFrame(columns=self.REQUIRED_COLUMNS)
        
        # å»é‡
        seen_urls = set()
        deduped_news = []
        for news in all_news_data:
            url = news.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                deduped_news.append(news)
        
        df = pd.DataFrame(deduped_news)
        
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨
        for col in self.REQUIRED_COLUMNS:
            if col not in df.columns:
                df[col] = ""
        
        # åªè¿”å›å¿…éœ€åˆ—ï¼Œæ—¶é—´ç­›é€‰ç”± normalize_dataframe ç»Ÿä¸€å¤„ç†
        df = df[self.REQUIRED_COLUMNS].copy()
        
        logger.info(f"æˆåŠŸè·å–åŒèŠ±é¡ºæ–°é—»åŸå§‹æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•ï¼ˆå»é‡åï¼Œæ—¶é—´ç­›é€‰ç”± normalize_dataframe ç»Ÿä¸€å¤„ç†ï¼‰")
        return df


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    crawler = ThxNewsCrawl(max_pages=2, enable_frontend_crawl=False)
    df = asyncio.run(crawler.fetch_data_async("2025-01-20 15:00:00"))
    print(f"è·å–åˆ° {len(df)} æ¡è®°å½•")
    if not df.empty:
        print(df.head())

